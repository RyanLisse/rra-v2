Okay, we've successfully integrated multimodal retrieval, allowing the chat to surface relevant images alongside text. Now, let's enhance the chat interface itself as per the PRD, focusing on:

1.  **Interactive Visual Citations:** Making the displayed sources (both text and image) more interactive, perhaps with hover cards or modals showing more detail.
2.  **Follow-up Questions:** AI-generated conversation continuations.
3.  **Conversation Management (Foundation):** Basic ability to list/select previous conversations (we'll defer full save/resume/drafts for a dedicated slice if this becomes too large).

This slice will be frontend-heavy, leveraging Shadcn UI components.

---

### Slice 11: Enhanced Chat Interface - Interactive Citations & Follow-up Questions

**What You're Building:**
*   Modifying the chat message component to make text and image citations more interactive (e.g., hover cards using Shadcn's `HoverCard` or `Popover` for text, clicking an image might open a larger preview in a `Dialog`).
*   Integrating a feature where the LLM suggests a few follow-up questions after its response.
*   (Light) Displaying a list of past conversation IDs to switch between (actual loading/saving of conversations will be a later slice).

**Tasks:**

1.  **Interactive Text Citations** - Complexity: 3
    *   [ ] In your chat message rendering component (where sources are displayed):
        *   For each text `ChatSource`:
            *   Wrap the displayed `contentSnippet` or a "View Source" link with Shadcn's `HoverCard` or `Popover` component.
            *   The content of the `HoverCard`/`Popover` should display more details from the `ChatSource` object, like the full `content` of the chunk (if fetched or available), `documentOriginalName`, `pageNumber` (if applicable), `similarityScore`.
    *   **Subtask 1.1:** Install/ensure `HoverCard` and/or `Popover` Shadcn components are available: `bunx shadcn-ui@latest add hover-card popover`.
    *   **Subtask 1.2:** Modify the text source rendering logic to use `HoverCard` or `Popover`.
    *   **Subtask 1.3:** Populate the card/popover with detailed source information.
2.  **Interactive Image Citations (Preview)** - Complexity: 3
    *   [ ] For each image `ChatSource`:
        *   Make the displayed `<img>` tag clickable.
        *   On click, open a Shadcn `Dialog` component to show a larger preview of the image.
        *   The dialog could also display metadata like `documentOriginalName`, `pageNumber`.
    *   **Subtask 2.1:** Install/ensure `Dialog` Shadcn component: `bunx shadcn-ui@latest add dialog`.
    *   **Subtask 2.2:** Wrap the image source rendering logic to trigger a `Dialog` on click.
    *   **Subtask 2.3:** Display the larger image and metadata in the `Dialog`.
3.  **AI-Generated Follow-up Questions** - Complexity: 4
    *   [ ] **Backend (`app/api/chat/route.ts`):**
        *   After the main LLM response is generated, make another (non-streaming) call to the LLM.
        *   Prompt it to suggest 2-3 relevant follow-up questions based on the original query, the retrieved context, and the main answer it just provided.
        *   Example prompt for follow-ups:
            ```
            Original Query: [User's Query]
            Retrieved Context: [Context used for main answer]
            Main Answer: [LLM's main answer]

            Based on the above, suggest 3 concise and relevant follow-up questions a user might ask next. Return them as a JSON array of strings. Example: ["What is X?", "How does Y work?", "Tell me more about Z."]
            ```
        *   Parse the JSON array of follow-up questions.
        *   Send these follow-up questions to the client as part of the `experimental_streamData` (e.g., as another `1:jsonData` chunk, or add them to the same JSON object as the sources).
    *   **Subtask 3.1:** Implement the backend logic to make a second LLM call for follow-up questions.
    *   **Subtask 3.2:** Define a new structure within `experimental_streamData` to hold these follow-up questions (e.g., add a `followUps: string[]` field to the JSON sent with sources).
    *   **Frontend (Chat Message Component):**
        *   Parse the follow-up questions from `message.experimental_streamData`.
        *   Display these questions as clickable buttons or links below the AI's response.
        *   When a follow-up question is clicked, it should populate the chat input field and (optionally) auto-submit.
    *   **Subtask 3.3:** Update frontend to parse and display follow-up questions.
    *   **Subtask 3.4:** Implement click functionality for follow-up questions.
4.  **Basic Conversation List (Display Only)** - Complexity: 2
    *   [ ] The Vercel AI Chatbot template already has a sidebar for conversations. We'll leverage this.
    *   [ ] For now, when a new chat starts (e.g., a new `id` is generated by `useChat`), simply add this `id` to a local state array representing conversation IDs.
    *   [ ] Display this list of IDs in the sidebar. Clicking an ID won't load the conversation yet, but it sets the stage.
    *   [ ] This is a very light touch on "Conversation Management" just to acknowledge it. Full save/load is a bigger slice.
    *   **Note:** `useChat` hook from `ai/react` provides a unique `id` for each chat session.
5.  **Write Tests** - Complexity: 2
    *   [ ] **Frontend:**
        *   Test that `HoverCard`/`Popover`/`Dialog` components for sources are rendered and display correct information (using mocked source data).
        *   Test that follow-up questions are displayed and clicking them updates the input.
        *   Test that new chat IDs appear in the conversation list.

**Code Example (Interactive Text Citations - `ChatMessage` component partial):**
```typescript
// components/chat-message.tsx (or similar)
// ... (imports: Message type, ChatSource type, HoverCard, Popover, PopoverTrigger, PopoverContent from shadcn)

// if (source.type === 'text' && source.contentSnippet) {
//   return (
//     <HoverCard key={source.id}>
//       <HoverCardTrigger asChild>
//         <span className="text-blue-600 hover:underline cursor-pointer">
//           [Source: {source.documentOriginalName}, Page {source.pageNumber || 'N/A'}]
//         </span>
//       </HoverCardTrigger>
//       <HoverCardContent className="w-96"> {/* Adjust width as needed */}
//         <div className="space-y-2">
//           <h4 className="font-semibold">{source.documentOriginalName}</h4>
//           {source.pageNumber && <p className="text-sm">Page: {source.pageNumber}</p>}
//           <p className="text-sm">
//             {/* Ideally, you'd have the full chunk content here if it's not too long */}
//             {source.contentSnippet} {/* Or the full chunk content */}
//           </p>
//           {source.similarityScore && (
//             <p className="text-xs text-gray-500">Similarity: {source.similarityScore.toFixed(2)}</p>
//           )}
//         </div>
//       </HoverCardContent>
//     </HoverCard>
//   );
// }
```

**Code Example (Interactive Image Citations - `ChatMessage` component partial):**
```typescript
// components/chat-message.tsx (or similar)
// ... (imports: Dialog, DialogTrigger, DialogContent, DialogHeader, DialogTitle from shadcn)
// const [isPreviewOpen, setIsPreviewOpen] = useState(false); // If managing dialog state per image

// if (source.type === 'image' && source.imagePath) {
//   const imageUrl = `/api/images/${source.imagePath}`; // Ensure URL is correctly formed
//   return (
//     <Dialog> {/* Or manage open state with useState if multiple images per message */}
//       <DialogTrigger asChild>
//         <img
//           src={imageUrl}
//           alt={`Visual source from ${source.documentOriginalName}`}
//           className="max-w-[100px] max-h-[100px] inline-block cursor-pointer hover:opacity-80 m-1"
//         />
//       </DialogTrigger>
//       <DialogContent className="sm:max-w-[70vw] md:max-w-[60vw] lg:max-w-[50vw] xl:max-w-[40vw]"> {/* Responsive width */}
//         <DialogHeader>
//           <DialogTitle>
//             Image from: {source.documentOriginalName} (Page {source.pageNumber || 'N/A'})
//           </DialogTitle>
//         </DialogHeader>
//         <img src={imageUrl} alt={`Preview of source from ${source.documentOriginalName}`} className="w-full h-auto rounded-md" />
//       </DialogContent>
//     </Dialog>
//   );
// }
```

**Code Example (Backend for Follow-up Questions - `app/api/chat/route.ts` partial):**
```typescript
// app/api/chat/route.ts
// ... (existing imports, retrieveContextAndSources function)
import { generateText } from 'ai'; // For the non-streaming follow-up call

// ... inside POST handler, after main LLM stream is initiated or completed ...

// After streaming main response (or in parallel if carefully managed)
// Let's assume `fullLLMResponseText` contains the complete text from the main answer
// And `retrievedContextText` is the context used.
// And `lastUserMessage.content` is the user's query.

// This needs to be done carefully to not block the primary streaming response.
// Ideally, this happens after the main text stream is finished, and follow-ups are appended
// to the `1:jsonData` stream.

// Modify the manual stream construction in POST:
// ... (inside the customStream's start or pull method)
// async start(controller) {
//   // Stream LLM text tokens for main answer
//   // ...
//   // (Collect fullLLMResponseText as it streams for the follow-up prompt)
//   let fullLLMResponseText = "";
//   for await (const textPart of llmResultStream.textStream) {
//     controller.enqueue(textEncoder.encode(`0:${JSON.stringify(textPart)}\n`));
//     fullLLMResponseText += textPart;
//   }

//   // Now, generate follow-up questions
//   let followUpQuestions: string[] = [];
//   if (lastUserMessage?.content && fullLLMResponseText) {
//     try {
//       const followUpPrompt = `Original Query: ${lastUserMessage.content}\nRetrieved Context: ${retrievedContextText}\nMain Answer: ${fullLLMResponseText}\n\nBased on the above, suggest 2-3 concise and relevant follow-up questions a user might ask next. Return them as a JSON array of strings. Example: ["What is X?", "How does Y work?"]`;
//       const { text: followUpJson } = await generateText({
//         model: google('gemini-1.5-flash-latest'), // Or your preferred model
//         prompt: followUpPrompt,
//       });
//       followUpQuestions = JSON.parse(followUpJson || "[]");
//     } catch (e) {
//       console.error("Failed to generate or parse follow-up questions:", e);
//     }
//   }

//   // Combine sources and follow-ups into one data object
//   const finalDataPayload = {
//     sources: sourcesForMessage,
//     followUps: followUpQuestions,
//   };

//   if (sourcesForMessage.length > 0 || followUpQuestions.length > 0) {
//     const jsonData = JSON.stringify(finalDataPayload);
//     controller.enqueue(textEncoder.encode(`1:${jsonData}\n`));
//   }
//   controller.close();
// }
```

**Frontend (Chat Message Component - Displaying Follow-ups):**
```typescript
// components/chat-message.tsx (or similar)
// ... (parsing sources from experimental_streamData)
// let followUps: string[] | undefined = undefined;
// if (isAIMessage && experimental_streamData && typeof experimental_streamData === 'object' && experimental_streamData !== null) {
//    // Assuming experimental_streamData is the object { sources: [], followUps: [] }
//    sources = (experimental_streamData as any).sources;
//    followUps = (experimental_streamData as any).followUps;
// }

// ... in the JSX, after rendering sources ...
// {isAIMessage && followUps && followUps.length > 0 && (
//   <div className="follow-ups mt-2 pt-2 border-t">
//     <p className="text-sm font-medium mb-1">Suggested follow-ups:</p>
//     <div className="flex flex-wrap gap-2">
//       {followUps.map((q, i) => (
//         <Button
//           key={i}
//           variant="outline"
//           size="sm"
//           onClick={() => {
//             setInput(q); // setInput from useChat
//             // Optionally, trigger submit: handleSubmit(new Event('submit')); but might need form ref
//           }}
//         >
//           {q}
//         </Button>
//       ))}
//     </div>
//   </div>
// )}
```

**Frontend (Conversation List - `components/sidebar.tsx` or similar in Vercel AI Chatbot):**
```typescript
// components/sidebar.tsx (or where conversation list is managed)
// 'use client'; // If it's a client component
// import { useChat } from 'ai/react'; // Or access chat state via context/props
// import { useEffect, useState } from 'react';
// import { nanoid } from 'nanoid'; // If generating IDs manually, useChat provides one

// // This is a simplified example. The Vercel AI Chatbot template has more robust state management.
// // You'd typically lift the conversations state up or use a global state manager.
// export function ChatSidebar() {
//   // const { chatId } = useChat(); // This doesn't give a list.
//   // You'd need to manage a list of chat sessions.
//   // The Vercel template (`app/actions.ts` `getChats`, `removeChat`, etc.) handles this.
//   // For this slice, we can just show how to *add* to a conceptual list.

//   // Let's assume `chats` is an array of {id: string, title: string} passed as a prop
//   // or fetched via a server action as in the Vercel template.
//   // const chats = await getChats(userId); // Server action example

//   // If just using useChat's ID:
//   // You would need a way to store these IDs persistently or in a higher-level state.
//   // For now, this part is more about UI readiness.
//   // The Vercel AI Chatbot already has a sidebar that lists chats.
//   // When a new chat is created (e.g., by clearing messages or starting fresh),
//   // its `id` (from `useChat` or a newly generated one) should appear in that list.
//   // The existing template's `app/actions.ts` and `useLocalStorage` for `newChatId`
//   // already handle much of this. We just need to ensure new chats get listed.
//   // No new code might be needed here if using the template's structure well.
// }
```The Vercel AI Chatbot template already has a sophisticated sidebar. The main task here is ensuring that when a new chat session starts (which `useChat` handles by generating an `id`), this new session appears in the list. The template's `actions.tsx` (`getChats`, `saveChat`) and the way it manages `chatId` via `usePathname` and local storage are key here.

**Ready to Merge Checklist:**
*   [ ] Text citations use Shadcn `HoverCard` or `Popover` to show more details on hover/click.
*   [ ] Image citations use Shadcn `Dialog` to show a larger preview on click.
*   [ ] Backend Chat API generates relevant follow-up questions using a second LLM call.
*   [ ] Follow-up questions are streamed to the client via `experimental_streamData`.
*   [ ] Frontend chat displays follow-up questions as clickable buttons/links that populate the input field.
*   [ ] (Light) New chat sessions (identified by their `id` from `useChat`) are conceptually ready to be listed in the sidebar (leveraging existing Vercel AI Chatbot structure).
*   [ ] All tests pass (bun test).
*   [ ] Linting passes (bun run lint).
*   [ ] Build succeeds (bun run build).
*   [ ] Code reviewed by senior dev.
*   [ ] Feature works as expected: Citations are interactive, and AI suggests follow-up questions.

**Quick Research (5-10 minutes):**
*   **Shadcn UI `HoverCard`, `Popover`, `Dialog`:** Review their documentation for API and examples. [https://ui.shadcn.com/docs/components/hover-card](https://ui.shadcn.com/docs/components/hover-card), [https://ui.shadcn.com/docs/components/popover](https://ui.shadcn.com/docs/components/popover), [https://ui.shadcn.com/docs/components/dialog](https://ui.shadcn.com/docs/components/dialog).
*   **Prompting LLMs for JSON output:** Techniques to reliably get JSON arrays.
*   **Vercel AI SDK `generateText`:** For non-streaming LLM calls.
*   **Managing state for multiple interactive elements in React:** (e.g., if multiple dialogs/popovers need independent open states).

**Need to Go Deeper?**
*   **Research Prompt:** *"I want to display a list of sources in a React chat message, where each text source shows a detailed popover on hover, and each image source opens a modal dialog with a larger preview on click. Using Shadcn UI components (`HoverCard`, `Dialog`), show how to implement this, including managing the open/closed state for these interactive elements, especially if there are multiple sources per message."*
*   **Research Prompt:** *"How can I prompt an LLM (like Gemini) to generate a list of follow-up questions based on a conversation history and ensure it returns the output as a valid JSON array of strings? What are common issues and how can I make the parsing on the backend more robust?"*

**Questions for Senior Dev:**
*   [ ] For follow-up questions, is a second, separate LLM call the best approach, or could we try to get this in the primary streaming response (e.g., by instructing the LLM to end its main answer with a special token, then follow-ups)? (Separate call is simpler to manage initially).
*   [ ] How complex should the "full chunk content" in the text citation popover be? Should we fetch it on demand if it's very long? (For now, using the snippet or a reasonable cap is fine).
*   [ ] The conversation list is very basic. What's the priority for implementing full save/load/rename/delete functionality from the PRD? (This will likely be its own slice).

---

This slice significantly polishes the user experience, making the chat more interactive, trustworthy, and helpful in guiding the conversation.