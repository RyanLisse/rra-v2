Okay, we've established basic monitoring and logging for our Inngest workflows. The PRD has a strong emphasis on **Quality & Testing**, specifically mentioning:
*   "Test Coverage: 80%+ with unit, integration, and RAG evaluation tests"
*   "DeepEval Integration: Semantic similarity and LLM rubric testing with visual content"
*   "TDD Approach: Test-first development for core components"

While achieving 80%+ coverage and full DeepEval integration is a large undertaking, this slice will focus on **setting up DeepEval and writing initial RAG evaluation tests** for our existing text-based RAG pipeline. This is a specialized form of testing crucial for AI applications.

---

### Slice 20: Initial RAG Evaluation with DeepEval

**What You're Building:**
*   Integrating DeepEval into the project.
*   Creating a small dataset of questions and expected contexts/answers based on a sample document.
*   Writing initial DeepEval tests to measure:
    *   Contextual Relevance (how relevant are the retrieved chunks to the query?)
    *   Faithfulness (does the LLM's answer stick to the provided context?)
    *   Answer Relevance (how relevant is the LLM's answer to the original query?)
*   Running these DeepEval tests as part of your testing suite.

**Prerequisites:**
*   You'll need an OpenAI API key for DeepEval's default LLM-based metrics, even if your main application uses Gemini. DeepEval uses LLMs to evaluate other LLMs/RAG systems. Configure `OPENAI_API_KEY` in your environment.
*   A sample document that has been processed through your pipeline (uploaded, text extracted, chunked, embedded).

**Tasks:**

1.  **Install DeepEval** - Complexity: 1
    *   `bun add deepeval`
2.  **Prepare a Small Evaluation Dataset** - Complexity: 3
    *   [ ] Choose one of your sample documents that has been fully processed by your pipeline (text extracted, chunked, embeddings stored).
    *   [ ] Manually create a small set (e.g., 5-10) of question-answer-context triplets:
        *   `query`: A question a user might ask about the document.
        *   `expected_output`: An ideal answer to the query, based *only* on the document.
        *   `actual_output`: This will be generated by your RAG system during the test.
        *   `retrieval_context`: A list of text snippets from your document that *should ideally* be retrieved to answer the query.
        *   `context`: This will be the actual context retrieved by your RAG system during the test.
    *   [ ] Store this dataset, perhaps as a JSON file (e.g., `test/rag-eval-dataset.json`) or directly in your test file.
    *   **Example entry:**
        ```json
        // {
        //   "query": "What is the main purpose of the Project X document?",
        //   "expected_output": "The main purpose of Project X is to outline the strategy for market expansion in Q4.",
        //   "retrieval_context": [
        //     "Project X aims to detail the comprehensive strategy for entering new markets in the fourth quarter...",
        //     "Key objectives include a 15% market share increase, facilitated by the initiatives described herein for Q4."
        //   ]
        // }
        ```
3.  **Create DeepEval Test File** - Complexity: 4
    *   [ ] Create a new test file, e.g., `test/rag.deepeval.test.ts`.
    *   [ ] You will need to write a function that takes a `query` (and a `documentId` for your sample doc) and programmatically runs it through your RAG pipeline:
        1.  Calls your search service (`searchAndRerank` or equivalent) to get retrieved chunks (this will be the `actual_context`).
        2.  Calls your chat API logic (or a refactored version of it) with the query and retrieved context to get the LLM's answer (this will be the `actual_output`).
    *   [ ] For each item in your evaluation dataset:
        *   Define a DeepEval test case using `@deepeval.test_case`.
        *   Use DeepEval metrics like `assert_contextual_relevance`, `assert_faithfulness`, `assert_answer_relevance`.
    *   **Subtask 3.1:** Implement the helper function to run a query through your RAG pipeline and get `actual_output` and `actual_context`. This might involve calling your API endpoints or refactoring backend logic to be callable directly.
    *   **Subtask 3.2:** Structure the DeepEval test file and define test cases for each dataset entry.
    *   **Subtask 3.3:** Integrate DeepEval metrics for contextual relevance, faithfulness, and answer relevance.
4.  **Configure DeepEval (API Keys, Environment)** - Complexity: 1
    *   [ ] Ensure `OPENAI_API_KEY` is set in your environment where you run the tests. DeepEval uses this for its LLM-based evaluations.
    *   [ ] You might need to set `DEEPEVAL_TELEMETRY_ENABLED=false` if you want to disable telemetry.
5.  **Run DeepEval Tests** - Complexity: 1
    *   [ ] DeepEval tests are typically run using pytest if you were in Python. For TypeScript/JS, Vitest can run these test files if structured correctly, or you might run them as a separate script.
    *   **To run with Vitest:** Ensure your DeepEval test functions are standard Vitest `test()` or `it()` blocks, and the DeepEval assertions are made within them. DeepEval's `@deepeval.test_case` decorator might require a specific test runner or adaptation.
    *   **Alternative (Dedicated Script):** You might have a `bun run test:deepeval` script that specifically executes DeepEval tests if they don't integrate smoothly with Vitest's default runner.
        *   The `deepeval` CLI itself has a `test run` command. You'd need to structure your tests in a way it expects (often Python files).
        *   **Focus for this slice:** Let's try to make it work within Vitest. If DeepEval's decorators are problematic, we can call DeepEval's metric functions directly.
        ```typescript
        // test/rag.deepeval.test.ts
        // import { test, assert } from 'deepeval'; // This is Python syntax
        // For JS/TS, you typically import specific metrics and test functions.
        // import { assertPass, assertFail, UnitTest } from 'deepeval'; // Might be different
        // import { AnswerRelevanceMetric, FaithfulnessMetric, ContextualRelevanceMetric } from 'deepeval/metrics';
        // This is evolving, check current DeepEval JS/TS SDK docs.

        // Let's assume a more direct usage of metrics for JS/TS within Vitest:
        // import { evaluate, TestEntry } from 'deepeval'; // Hypothetical JS SDK structure
        // import { AnswerRelevanceMetric, FaithfulnessMetric, ContextualRelevanceMetric } from 'deepeval/metrics';
        ```
    *   **Revised approach for JS/TS with DeepEval (refer to their latest JS SDK docs):**
        The JS SDK for DeepEval allows programmatic evaluation.
6.  **Analyze Initial Results** - Complexity: 2
    *   [ ] Review the output from DeepEval. Understand the scores for each metric.
    *   [ ] This will give you a baseline for your RAG system's quality on the chosen metrics.
    *   [ ] Don't expect perfect scores initially. This is about establishing a measurement process.

**Code Example (Conceptual `test/rag.deepeval.test.ts`):**
The DeepEval JS/TS SDK is newer and its patterns might differ slightly from the Python version which is more established. The core idea is to programmatically get your RAG output and then use DeepEval's metric classes.

```typescript
// test/rag.deepeval.test.ts
import { describe, it, expect, beforeAll } from 'vitest';
import {
  evaluate,
  // TestEntry, // This might be from an older version or Python
  // For JS, you often define test cases and then use metric objects.
} from 'deepeval'; // Assuming 'deepeval' is the correct JS package import for core functions
import {
  AnswerRelevanceMetric,
  FaithfulnessMetric,
  ContextualRelevanceMetric,
  // Other metrics like SummarizationMetric, BiasMetric etc.
} from 'deepeval/metrics'; // This is a common pattern for JS SDKs

// Helper function to simulate or call your RAG pipeline
// This needs to be implemented to actually call your backend
async function getRAGOutput(query: string, documentId: string): Promise<{ actual_output: string; retrieval_context: string[]; }> {
  // 1. Call your search service (/api/documents/search-chunks or direct function)
  //    to get retrieved_chunks (this will be the `retrieval_context` for DeepEval)
  //    Make sure to get the actual text content of these chunks.
  const searchResponse = await fetch(`${process.env.NEXT_PUBLIC_APP_URL}/api/documents/search-chunks`, { // Ensure URL is correct
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ query, documentId, topK: 5 }), // Or your desired topK for eval
  });
  if (!searchResponse.ok) throw new Error(`Search API failed: ${await searchResponse.text()}`);
  const searchResults = await searchResponse.json();
  const actualContextChunks: string[] = (searchResults.textResults || []).map((r: any) => r.content);

  // 2. Call your chat API (/api/chat or direct function) with the query and actualContextChunks
  //    to get the LLM's answer (this will be the `actual_output`)
  const chatPayload = {
    messages: [{ role: 'user', content: query }],
    data: { documentId }, // Pass documentId if your chat API uses it
    // You might need to manually construct the prompt with context here if not calling the full chat API
  };
  // For DeepEval, we often need to simulate the exact input to the LLM generation step.
  // So, if your chat API augments the prompt with `actualContextChunks`, replicate that.
  const systemPrompt = `System: ... DOCUMENT EXCERPTS:\n${actualContextChunks.join("\n---\n")} ---`;
  const messagesForLLM = [{role: 'system', content: systemPrompt}, {role: 'user', content: query}];

  // This part needs to call your LLM generation logic directly, not necessarily the full streaming chat API.
  // For simplicity, if your chat API can return a non-streamed JSON with the answer, use that.
  // Or, refactor the core LLM call logic to be reusable.
  // This is a placeholder for actually getting the LLM's generated answer.
  // const llmResponse = await generateText({ model: yourGoogleModel, messages: messagesForLLM });
  // const actualOutput = llmResponse.text;
  const actualOutput = "Placeholder LLM Answer based on context."; // Replace with actual LLM call

  return {
    actual_output: actualOutput,
    retrieval_context: actualContextChunks,
  };
}

// Your evaluation dataset
const ragEvalDataset = [
  {
    testCaseId: "project_x_purpose", // Optional ID for your reference
    query: "What is the main purpose of the Project X document?",
    expected_output: "The main purpose of Project X is to outline the strategy for market expansion in Q4.",
    // `retrieval_context` in DeepEval usually means the *actual* context retrieved by your system.
    // `expected_retrieval_context` or similar might be used if you want to compare ideal vs actual retrieval.
    // For now, we'll provide query and expected_output, and the RAG system generates actual_output and retrieval_context.
    // Some DeepEval metrics might also take an `expected_context` or `ground_truth_context`.
  },
  // ... more test cases
];

// Ensure OPENAI_API_KEY is set in your environment for DeepEval's metrics
if (!process.env.OPENAI_API_KEY) {
  console.warn("OPENAI_API_KEY not set. DeepEval metrics might fail or use a fallback.");
}
// You might also want to set DEEPEVAL_TELEMETRY_ENABLED=false

describe('RAG System Evaluation with DeepEval', () => {
  // Assume a sample document ID that has been processed
  const sampleDocumentId = 'your-sample-document-id-in-db';

  for (const testCase of ragEvalDataset) {
    it(`evaluates query: "${testCase.query.substring(0,30)}..."`, async () => {
      if (!process.env.OPENAI_API_KEY) {
        // Skip test or mark as pending if OpenAI key is not available, as DeepEval metrics rely on it.
        console.warn("Skipping DeepEval test due to missing OPENAI_API_KEY");
        return;
      }

      const { actual_output, retrieval_context } = await getRAGOutput(testCase.query, sampleDocumentId);

      // Define metrics to use for this test case
      // Thresholds are between 0 and 1. Higher is better.
      const answerRelevanceMetric = new AnswerRelevanceMetric({ threshold: 0.7, model: "gpt-4o" }); // Specify model if needed
      const faithfulnessMetric = new FaithfulnessMetric({ threshold: 0.7, model: "gpt-4o" });
      const contextualRelevanceMetric = new ContextualRelevanceMetric({ threshold: 0.6, model: "gpt-4o" });

      // DeepEval's `evaluate` function typically takes an array of test cases.
      // Here, we are evaluating one by one within Vitest's `it` block.
      // The JS SDK might have a slightly different pattern for `TestEntry` or direct metric calculation.
      // Let's assume direct metric calculation:

      // 1. Answer Relevance
      await answerRelevanceMetric.measureAsync({
        prediction: actual_output,
        instruction: testCase.query,
        // expectedResponse: testCase.expected_output, // Some metrics might use this
      });
      expect(answerRelevanceMetric.score).toBeGreaterThanOrEqual(answerRelevanceMetric.threshold);
      if (answerRelevanceMetric.reason) console.log(`Answer Relevance Reason: ${answerRelevanceMetric.reason}`);


      // 2. Faithfulness (does the answer stick to the provided context?)
      await faithfulnessMetric.measureAsync({
        prediction: actual_output,
        retrievalContext: retrieval_context, // Pass the actual retrieved context
        // instruction: testCase.query, // Optional, but good for context
      });
      expect(faithfulnessMetric.score).toBeGreaterThanOrEqual(faithfulnessMetric.threshold);
      if (faithfulnessMetric.reason) console.log(`Faithfulness Reason: ${faithfulnessMetric.reason}`);

      // 3. Contextual Relevance (is the retrieved context relevant to the query?)
      await contextualRelevanceMetric.measureAsync({
        retrievalContext: retrieval_context,
        instruction: testCase.query,
      });
      expect(contextualRelevanceMetric.score).toBeGreaterThanOrEqual(contextualRelevanceMetric.threshold);
      if (contextualRelevanceMetric.reason) console.log(`Contextual Relevance Reason: ${contextualRelevanceMetric.reason}`);

      // You can also use the global `evaluate` function if you structure as TestEntry array.
      // Example:
      // const testEntry = {
      //   input: testCase.query,
      //   actual_output: actual_output,
      //   expected_output: testCase.expected_output,
      //   context: retrieval_context,
      //   // retrieval_context: retrieval_context, // If different from context
      // };
      // const results = await evaluate([testEntry], [answerRelevanceMetric, faithfulnessMetric, contextualRelevanceMetric]);
      // results[0].metrics.forEach(metric => {
      //   expect(metric.score).toBeGreaterThanOrEqual(metric.threshold);
      // });

    }, 20000); // Increase timeout for LLM calls
  }
});
```
**Important Note on DeepEval JS/TS SDK:** The JS/TS SDK for DeepEval is an active area of development. The exact import paths, class names (`TestEntry`, metric classes), and method signatures (`measureAsync`, `evaluate`) might change. **Always refer to the latest official DeepEval JS/TS documentation for the most accurate usage.** The example above is based on common patterns seen in such SDKs.

**Ready to Merge Checklist:**
*   [ ] DeepEval package (`deepeval`) installed.
*   [ ] `OPENAI_API_KEY` configured in the environment for running DeepEval tests.
*   [ ] A small evaluation dataset (queries, expected outputs, ideal contexts) created for a sample document.
*   [ ] A helper function is implemented to programmatically run a query through the RAG pipeline (retrieval + generation) to get `actual_output` and `actual_context`.
*   [ ] DeepEval tests are written (e.g., in `test/rag.deepeval.test.ts`) using the dataset and the RAG helper function.
*   [ ] Tests use DeepEval metrics like `AnswerRelevanceMetric`, `FaithfulnessMetric`, and `ContextualRelevanceMetric`.
*   [ ] DeepEval tests can be executed (e.g., via `bun test` if integrated with Vitest, or a separate script) and produce scores.
*   [ ] Initial analysis of DeepEval results provides a baseline for RAG quality.
*   [ ] All tests (Vitest unit/integration + DeepEval RAG eval) pass.
*   [ ] Linting passes (bun run lint).
*   [ ] Build succeeds (bun run build).
*   [ ] Code reviewed by senior dev.

**Quick Research (5-10 minutes):**
*   **DeepEval Official Documentation (especially JS/TS SDK if available):** [https://docs.deepeval.com/](https://docs.deepeval.com/). Look for quickstarts, JS/TS examples, and metric definitions.
*   **DeepEval Metrics:** Understand what `Contextual Relevance`, `Faithfulness`, and `Answer Relevance` specifically measure and how they are calculated (often using an LLM like GPT).
*   **Structuring evaluation datasets for RAG.**

**Need to Go Deeper?**
*   **Research Prompt:** *"I want to use DeepEval's JS/TS SDK to evaluate my RAG application built with Next.js. Show me how to: 1. Define an evaluation dataset (queries, expected outputs). 2. Create a helper function that takes a query and returns the `actual_output` (LLM answer) and `retrieval_context` (retrieved chunks) from my RAG system. 3. Write a test script (e.g., for Vitest) that uses DeepEval's `AnswerRelevanceMetric`, `FaithfulnessMetric`, and `ContextualRelevanceMetric` to evaluate each case in my dataset. How do I interpret the scores and reasons provided by DeepEval?"*

**Questions for Senior Dev:**
*   [ ] DeepEval relies on an LLM (like GPT via OpenAI API) for its evaluations. What are the cost implications of running these tests frequently? (Each metric calculation can be an LLM call).
*   [ ] The PRD mentions "Semantic similarity and LLM rubric testing with visual content" for DeepEval. This slice focuses on text. How would we approach evaluating visual content relevance with DeepEval in a future slice? (DeepEval might have specific multimodal metrics, or we'd need to adapt).
*   [ ] How do we set appropriate thresholds for the DeepEval metrics? Is there a standard, or is it application-specific? (Often application-specific, established through iteration).
*   [ ] Integrating DeepEval tests that make external API calls (to OpenAI and your own app) into a standard unit/integration test runner like Vitest can be slow. What's the best practice for running these types of evaluation tests (e.g., separate suite, run less frequently)?

---

This slice introduces a critical aspect of building reliable AI applications: quantitative evaluation of your RAG pipeline's quality. While it's an initial setup, it provides a framework for ongoing monitoring and improvement of your system's accuracy and relevance.